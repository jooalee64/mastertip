{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOle57TyK5boAj95dfLf0qf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jooalee64/mastertip/blob/main/mastertip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1148b9f2",
        "outputId": "7a0a9692-f3dd-441f-afe9-11a075d0cb27"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mastertip_"
      ],
      "metadata": {
        "id": "h0hKDKSVUdd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "mastip2"
      ],
      "metadata": {
        "id": "-kE7hOOYbqBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: if you're on Colab, mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')\n",
        "\n",
        "# (1) Imports\n",
        "import os, math, json, glob, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision.ops import roi_align\n",
        "from torchvision.transforms import functional as TF\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "# (2) User paths — EDIT HERE if needed\n",
        "ROOT_DIR = \"/gdrive/MyDrive/boats\"\n",
        "print(\"ROOT_DIR:\", ROOT_DIR)\n",
        "\n",
        "# Image directories\n",
        "IMAGES_DIR = os.path.join(ROOT_DIR, \"yolo\", \"images\")\n",
        "TRAIN_IMAGES_DIR = os.path.join(IMAGES_DIR, \"train\")\n",
        "VAL_IMAGES_DIR   = os.path.join(IMAGES_DIR, \"val\")\n",
        "TEST_IMAGES_DIR  = os.path.join(IMAGES_DIR, \"test\")\n",
        "\n",
        "# Label directories — choose ONE of the following (uncomment as needed)\n",
        "\n",
        "# A) Standard YOLO structure\n",
        "LABELS_DIR = os.path.join(ROOT_DIR, \"yolo\", \"labels\")\n",
        "\n",
        "# B) Your custom storage path (uncomment to use)\n",
        "# LABELS_DIR = os.path.join(ROOT_DIR, \"yolo\", \"labels-storage\", \"storage-a-labels\", \"hull-labels\")\n",
        "\n",
        "TRAIN_LABELS_DIR = os.path.join(LABELS_DIR, \"train\")\n",
        "VAL_LABELS_DIR   = os.path.join(LABELS_DIR, \"val\")\n",
        "TEST_LABELS_DIR  = os.path.join(LABELS_DIR, \"test\")\n",
        "\n",
        "print(\"LABELS_DIR:\", LABELS_DIR)\n",
        "\n",
        "# Class IDs (adjust if your dataset uses different ids)\n",
        "HULL_CLASS_ID = 0  # hull box\n",
        "TIP_CLASS_ID  = 1  # tip encoded as tiny box (we use its center)\n",
        "\n",
        "# Training hyperparams (edit as you like)\n",
        "MAX_SIDE    = 1024\n",
        "BATCH_SIZE  = 4\n",
        "EPOCHS      = 5\n",
        "LR          = 1e-3\n",
        "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Cone-mask config for Stage-B\n",
        "CONE_CFG = dict(base_y_ratio=0.75, apex_y_ratio=0.05, base_width_ratio=0.35)\n",
        "\n",
        "# Utility\n",
        "def set_seed(seed: int = 1337):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(1337)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvvs_2ySbviS",
        "outputId": "02755fa6-d2d9-4332-bd15-927b65f293d8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROOT_DIR: /gdrive/MyDrive/boats\n",
            "LABELS_DIR: /gdrive/MyDrive/boats/yolo/labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils & transforms\n",
        "\n",
        "def to_tensor(img: Image.Image) -> torch.Tensor:\n",
        "    return TF.to_tensor(img)\n",
        "\n",
        "def resize_keep_aspect(img: Image.Image, max_side: int = 1024) -> Tuple[Image.Image, float]:\n",
        "    w, h = img.size\n",
        "    scale = 1.0\n",
        "    if max(w, h) > max_side:\n",
        "        scale = max_side / float(max(w, h))\n",
        "        img = img.resize((int(round(w * scale)), int(round(h * scale))), Image.BILINEAR)\n",
        "    return img, scale\n"
      ],
      "metadata": {
        "id": "1iHmkstpbxnI"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets (JSONL & YOLO dirs)\n",
        "\n",
        "@dataclass\n",
        "class Sample:\n",
        "    image_path: str\n",
        "    hulls: List[List[float]]\n",
        "    tips: List[List[float]]\n",
        "\n",
        "class MastTipJsonl(Dataset):\n",
        "    def __init__(self, jsonl_path: str, max_side: int = 1024, training: bool = True):\n",
        "        self.items: List[Sample] = []\n",
        "        with open(jsonl_path, 'r') as f:\n",
        "            for line in f:\n",
        "                j = json.loads(line)\n",
        "                self.items.append(Sample(j['image'], j['hulls'], j['tips']))\n",
        "        self.max_side = max_side\n",
        "        self.training = training\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        it = self.items[idx]\n",
        "        img = Image.open(it.image_path).convert('RGB')\n",
        "        img, scale = resize_keep_aspect(img, self.max_side)\n",
        "        w, h = img.size\n",
        "        img_t = to_tensor(img)\n",
        "\n",
        "        hulls = np.array(it.hulls, dtype=np.float32) * scale\n",
        "        tips = np.array(it.tips, dtype=np.float32) * scale\n",
        "\n",
        "        if self.training and random.random() < 0.5:\n",
        "            img_t = torch.flip(img_t, dims=[2])\n",
        "            hulls[:, 0] = (w - hulls[:, 0] - hulls[:, 2])\n",
        "            tips[:, 0] = (w - tips[:, 0])\n",
        "\n",
        "        target = {'hulls': torch.from_numpy(hulls),\n",
        "                  'tips': torch.from_numpy(tips),\n",
        "                  'scale': scale,\n",
        "                  'size': (h, w)}\n",
        "        return img_t, target\n",
        "\n",
        "def _parse_yolo_label(label_path: str, img_w: int, img_h: int):\n",
        "    hulls: List[List[float]] = []\n",
        "    tips_xy: List[List[float]] = []\n",
        "    if not os.path.exists(label_path):\n",
        "        return hulls, tips_xy\n",
        "    with open(label_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                continue\n",
        "            cls = int(float(parts[0]))\n",
        "            x_c, y_c, w_n, h_n = map(float, parts[1:])\n",
        "            x_c *= img_w; y_c *= img_h; w = w_n * img_w; h = h_n * img_h\n",
        "            x = x_c - w / 2.0; y = y_c - h / 2.0\n",
        "            if cls == HULL_CLASS_ID:\n",
        "                hulls.append([x, y, w, h])\n",
        "            elif cls == TIP_CLASS_ID:\n",
        "                tips_xy.append([x_c, y_c])  # center as tip\n",
        "    return hulls, tips_xy\n",
        "\n",
        "class MastTipYoloDirs(Dataset):\n",
        "    def __init__(self, images_dir: str, labels_dir: str, max_side: int = 1024, training: bool = True):\n",
        "        self.images = []\n",
        "        exts = ('*.jpg', '*.jpeg', '*.png', '*.bmp')\n",
        "        for e in exts:\n",
        "            self.images.extend(sorted(glob.glob(os.path.join(images_dir, e))))\n",
        "        self.labels_dir = labels_dir\n",
        "        self.max_side = max_side\n",
        "        self.training = training\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        label_path = os.path.join(self.labels_dir, base + '.txt')\n",
        "\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        w0, h0 = img.size\n",
        "        hulls, tips = _parse_yolo_label(label_path, w0, h0)\n",
        "\n",
        "        # naive 1:1 pairing\n",
        "        n = min(len(hulls), len(tips))\n",
        "        hulls = hulls[:n]; tips = tips[:n]\n",
        "        if n == 0:\n",
        "            img, _ = resize_keep_aspect(img, self.max_side)\n",
        "            return to_tensor(img), {'hulls': torch.zeros((0,4), dtype=torch.float32),\n",
        "                                    'tips': torch.zeros((0,2), dtype=torch.float32),\n",
        "                                    'scale': 1.0, 'size': img.size[::-1]}\n",
        "\n",
        "        img, scale = resize_keep_aspect(img, self.max_side)\n",
        "        w, h = img.size\n",
        "        img_t = to_tensor(img)\n",
        "\n",
        "        hulls = (np.array(hulls, dtype=np.float32) * scale)\n",
        "        tips  = (np.array(tips,  dtype=np.float32) * scale)\n",
        "\n",
        "        if self.training and random.random() < 0.5:\n",
        "            img_t = torch.flip(img_t, dims=[2])\n",
        "            hulls[:, 0] = (w - hulls[:, 0] - hulls[:, 2])\n",
        "            tips[:, 0]  = (w - tips[:, 0])\n",
        "\n",
        "        target = {'hulls': torch.from_numpy(hulls),\n",
        "                  'tips': torch.from_numpy(tips),\n",
        "                  'scale': scale,\n",
        "                  'size': (h, w)}\n",
        "        return img_t, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs = [b[0] for b in batch]\n",
        "    tgts = [b[1] for b in batch]\n",
        "    return imgs, tgts\n"
      ],
      "metadata": {
        "id": "zDTiiTyLbzNr"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Components (Backbone, Tip head, Cone mask, StageB)\n",
        "\n",
        "class BackboneResNet50(nn.Module):\n",
        "    def __init__(self, pretrained: bool = True):\n",
        "        super().__init__()\n",
        "        m = resnet50(weights=(ResNet50_Weights.IMAGENET1K_V2 if pretrained else None))\n",
        "        self.stem = nn.Sequential(m.conv1, m.bn1, m.relu, m.maxpool)\n",
        "        self.layer1 = m.layer1\n",
        "        self.layer2 = m.layer2\n",
        "        self.layer3 = m.layer3  # stride 16\n",
        "        self.proj = nn.Conv2d(1024, 256, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "def gaussian_nll_2d(mu, log_sigma, rho_raw, y, reduction=\"mean\", eps=1e-6):\n",
        "    sigma = torch.exp(log_sigma) + eps\n",
        "    rho = torch.tanh(rho_raw).clamp(-0.999, 0.999)\n",
        "    dx = (y[:, 0] - mu[:, 0]) / sigma[:, 0]\n",
        "    dy = (y[:, 1] - mu[:, 1]) / sigma[:, 1]\n",
        "    rho_sq = rho[:, 0] ** 2\n",
        "    one_m_rho_sq = 1.0 - rho_sq + eps\n",
        "    q = (dx * dx - 2 * rho[:, 0] * dx * dy + dy * dy) / one_m_rho_sq\n",
        "    log_det = 2 * log_sigma[:, 0] + 2 * log_sigma[:, 1] + torch.log(one_m_rho_sq)\n",
        "    nll = 0.5 * (log_det + q + math.log(2.0 * math.pi))\n",
        "    return nll.mean() if reduction == \"mean\" else nll\n",
        "\n",
        "class TipHeadGaussian(nn.Module):\n",
        "    def __init__(self, in_channels=256, hidden=128):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(hidden, hidden, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x).flatten(1)\n",
        "        out = self.fc(h)\n",
        "        mu = out[:, :2]\n",
        "        log_sigma = out[:, 2:4]\n",
        "        rho_raw = out[:, 4:5]\n",
        "        return mu, log_sigma, rho_raw\n",
        "\n",
        "def build_cone_mask(h, w, base_y, base_xc, base_half_width, apex_y):\n",
        "    yy, xx = torch.meshgrid(torch.arange(h), torch.arange(w), indexing='ij')\n",
        "    denom = max(1, (base_y - apex_y))\n",
        "    t = (yy.float() - apex_y) / denom\n",
        "    t = torch.clamp(t, 0.0, 1.0)\n",
        "    halfw = (1.0 - t) * 1 + t * base_half_width\n",
        "    mask = (torch.abs(xx.float() - base_xc) <= halfw) & (yy >= apex_y) & (yy <= base_y)\n",
        "    return mask.float()\n",
        "\n",
        "class StageBTipNet(nn.Module):\n",
        "    def __init__(self, roi_out=28, pretrained_backbone=True):\n",
        "        super().__init__()\n",
        "        self.backbone = BackboneResNet50(pretrained=pretrained_backbone)\n",
        "        self.tip_head = TipHeadGaussian(in_channels=256, hidden=128)\n",
        "        self.roi_size = roi_out\n",
        "        self.spatial_scale = 1.0 / 16.0\n",
        "\n",
        "    def forward(self, images: List[torch.Tensor], rois_xywh: List[torch.Tensor], cone_cfg: Dict[str, float]):\n",
        "        device = images[0].device\n",
        "        imgs = torch.stack(images, dim=0)\n",
        "        feats = self.backbone(imgs)\n",
        "\n",
        "        roi_list = []\n",
        "        batch_index = []\n",
        "        for b, boxes in enumerate(rois_xywh):\n",
        "            if boxes.numel() == 0:\n",
        "                continue\n",
        "            xyxy = boxes.clone()\n",
        "            xyxy[:, 2] = xyxy[:, 0] + xyxy[:, 2]\n",
        "            xyxy[:, 3] = xyxy[:, 1] + xyxy[:, 3]\n",
        "            bidx = torch.full((xyxy.size(0), 1), b, device=xyxy.device)\n",
        "            roi_list.append(torch.cat([bidx, xyxy], dim=1))\n",
        "            batch_index.extend([b] * xyxy.size(0))\n",
        "        if len(roi_list) == 0:\n",
        "            return None\n",
        "        rois = torch.cat(roi_list, dim=0)\n",
        "\n",
        "        crops = roi_align(\n",
        "            feats, rois,\n",
        "            output_size=(self.roi_size // 4, self.roi_size // 4),\n",
        "            spatial_scale=self.spatial_scale,\n",
        "            sampling_ratio=2,\n",
        "            aligned=True,\n",
        "        )\n",
        "        crops_up = F.interpolate(crops, size=(self.roi_size, self.roi_size), mode='bilinear', align_corners=False)\n",
        "\n",
        "        N = crops_up.size(0)\n",
        "        masked = []\n",
        "        base_y_ratio = cone_cfg.get('base_y_ratio', 0.75)\n",
        "        apex_y_ratio = cone_cfg.get('apex_y_ratio', 0.05)\n",
        "        base_width_ratio = cone_cfg.get('base_width_ratio', 0.35)\n",
        "        for i in range(N):\n",
        "            H, W = self.roi_size, self.roi_size\n",
        "            base_y = int(round(base_y_ratio * (H - 1)))\n",
        "            apex_y = int(round(apex_y_ratio * (H - 1)))\n",
        "            base_xc = W // 2\n",
        "            base_halfw = int(round(base_width_ratio * (W / 2)))\n",
        "            m = build_cone_mask(H, W, base_y, base_xc, base_half_width=base_halfw, apex_y=apex_y).to(crops_up.device)\n",
        "            masked.append(crops_up[i] * m.unsqueeze(0))\n",
        "        masked = torch.stack(masked, dim=0)\n",
        "\n",
        "        mu, log_sigma, rho_raw = self.tip_head(masked)\n",
        "        aux = {'masked': masked, 'crops': crops_up, 'rois_batched': rois, 'batch_index': torch.tensor(batch_index, device=device)}\n",
        "        return mu, log_sigma, rho_raw, aux\n"
      ],
      "metadata": {
        "id": "905l_S5bb1C2"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training utilities\n",
        "\n",
        "def to_roi_coords(points_xy: torch.Tensor, boxes_xywh: torch.Tensor, roi_size: int) -> torch.Tensor:\n",
        "    x = (points_xy[:, 0] - boxes_xywh[:, 0]) / boxes_xywh[:, 2] * roi_size\n",
        "    y = (points_xy[:, 1] - boxes_xywh[:, 1]) / boxes_xywh[:, 3] * roi_size\n",
        "    return torch.stack([x, y], dim=1)\n",
        "\n",
        "def train_stage_b(model: 'StageBTipNet', loader: DataLoader, optimizer: torch.optim.Optimizer,\n",
        "                  device: str = 'cuda', epochs: int = 5, cone_cfg: Optional[Dict[str, float]] = None,\n",
        "                  log_every: int = 20):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    cone_cfg = cone_cfg or {}\n",
        "    global_step = 0\n",
        "    for ep in range(epochs):\n",
        "        for imgs, tgts in loader:\n",
        "            imgs = [im.to(device) for im in imgs]\n",
        "            rois = [t['hulls'].to(device).float() for t in tgts]\n",
        "            tips = [t['tips'].to(device).float() for t in tgts]\n",
        "            if sum([r.size(0) for r in rois]) == 0:\n",
        "                continue\n",
        "            tips_flat = torch.cat(tips, dim=0)\n",
        "            rois_flat = torch.cat(rois, dim=0)\n",
        "\n",
        "            out = model(imgs, rois, cone_cfg)\n",
        "            if out is None:\n",
        "                continue\n",
        "            mu, log_sigma, rho_raw, aux = out\n",
        "\n",
        "            y_roi = to_roi_coords(tips_flat.to(mu.device), rois_flat.to(mu.device), model.roi_size)\n",
        "            loss = gaussian_nll_2d(mu, log_sigma, rho_raw, y_roi, reduction='mean')\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            if global_step % log_every == 0:\n",
        "                print(f\"epoch {ep:02d} step {global_step:05d} | loss {loss.item():.4f}\")\n",
        "            global_step += 1\n"
      ],
      "metadata": {
        "id": "TsHnsen9b3gc"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference helpers & visualization\n",
        "\n",
        "def run_yolo_detect(image_path: str, conf: float = 0.25) -> List[List[float]]:\n",
        "    \"\"\"Optional: Use Ultralytics YOLO if installed. Returns hull boxes [x,y,w,h].\"\"\"\n",
        "    try:\n",
        "        from ultralytics import YOLO\n",
        "    except Exception:\n",
        "        print(\"[INFO] Ultralytics not installed; returning empty boxes.\")\n",
        "        return []\n",
        "    # Load a box detector; replace with your fine-tuned hull model path\n",
        "    # Using the user-provided path for the hull detector model\n",
        "    model_path = \"/gdrive/MyDrive/boats/yolo/models/hull-detector/yolo-hull-detector.pt\"\n",
        "    try:\n",
        "        model = YOLO(model_path)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Could not load YOLO model from {model_path}: {e}\")\n",
        "        print(\"Returning empty boxes.\")\n",
        "        return []\n",
        "\n",
        "    res = model(image_path, conf=conf, verbose=False)[0]\n",
        "    boxes_xyxy = res.boxes.xyxy.cpu().numpy().astype(np.float32)\n",
        "    hulls = []\n",
        "    for x1, y1, x2, y2 in boxes_xyxy:\n",
        "        hulls.append([float(x1), float(y1), float(x2 - x1), float(y2 - y1)])\n",
        "    return hulls\n",
        "\n",
        "def infer_stage_b(model: 'StageBTipNet', image_path: str, hulls_xywh: List[List[float]],\n",
        "                  device: str = 'cuda', cone_cfg: Optional[Dict[str, float]] = None) -> List[Dict]:\n",
        "    model.eval()\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img, _ = resize_keep_aspect(img, max_side=1024)\n",
        "    img_t = to_tensor(img).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model([img_t], [torch.tensor(hulls_xywh, device=device).float()], cone_cfg or {})\n",
        "        if out is None:\n",
        "            return []\n",
        "        mu, log_sigma, rho_raw, aux = out\n",
        "        # Convert mu from RoI coords to image coords\n",
        "        rois = aux['rois_batched'][:, 1:5]\n",
        "        boxes = rois.clone()\n",
        "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
        "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
        "        mu_img_x = mu[:, 0] / model.roi_size * boxes[:, 2] + boxes[:, 0]\n",
        "        mu_img_y = mu[:, 1] / model.roi_size * boxes[:, 3] + boxes[:, 1]\n",
        "        sigma = torch.exp(log_sigma)\n",
        "        rho = torch.tanh(rho_raw).clamp(-0.999, 0.999)\n",
        "        out_list = []\n",
        "        for i in range(mu.size(0)):\n",
        "            out_list.append({'tip_xy': (float(mu_img_x[i].item()), float(mu_img_y[i].item())),\n",
        "                             'sigma': (float(sigma[i, 0].item()), float(sigma[i, 1].item())),\n",
        "                             'rho': float(rho[i, 0].item()),\n",
        "                             'box_xywh': [float(x) for x in boxes[i].tolist()]})\n",
        "        return out_list\n",
        "\n",
        "def draw_predictions(image_path: str, preds: List[Dict], save_path: str):\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    dr = ImageDraw.Draw(img)\n",
        "    for p in preds:\n",
        "        x, y, w, h = p['box_xywh']\n",
        "        dr.rectangle([x, y, x + w, y + h], outline=(0, 255, 0), width=2)\n",
        "        tx, ty = p['tip_xy']; r = 3\n",
        "        dr.ellipse([tx - r, ty - r, tx + r, ty + r], outline=(255, 0, 0), width=2)\n",
        "    img.save(save_path)\n",
        "    print(\"Saved:\", save_path)"
      ],
      "metadata": {
        "id": "S1eBN8N3b5x8"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build dataset & train Stage-B using YOLO dirs\n",
        "\n",
        "train_ds = MastTipYoloDirs(images_dir=TRAIN_IMAGES_DIR, labels_dir=TRAIN_LABELS_DIR, max_side=MAX_SIDE, training=True)\n",
        "print(\"Num train images:\", len(train_ds))\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "model = StageBTipNet(roi_out=28, pretrained_backbone=True).to(DEVICE)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "\n",
        "train_stage_b(model, train_loader, opt, device=DEVICE, epochs=EPOCHS, cone_cfg=CONE_CFG, log_every=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywPq-ixXb7ph",
        "outputId": "06e316a5-ebe0-4dbb-d662-9da81f0cda93"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num train images: 1050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test inference on multiple images.\n",
        "# 1) Set a list of image paths from your dataset:\n",
        "test_images = []\n",
        "\n",
        "# e.g., pick the first few training images if available\n",
        "if len(train_ds) > 0:\n",
        "    # You can adjust the number of images to test here (e.g., [:5] for the first 5)\n",
        "    test_images = train_ds.images[:3]\n",
        "\n",
        "print(\"Test images:\", test_images)\n",
        "\n",
        "# 2) Choose how to get hulls: (A) use YOLO detect (needs ultralytics), or (B) make a center box\n",
        "USE_YOLO = True\n",
        "\n",
        "output_dir = \"/tmp/mastertip_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "if test_images:\n",
        "    for i, test_img in enumerate(test_images):\n",
        "        print(f\"\\nProcessing image {i+1}/{len(test_images)}: {test_img}\")\n",
        "        if USE_YOLO:\n",
        "            hulls = run_yolo_detect(test_img)  # list of [x,y,w,h]\n",
        "            if not hulls:\n",
        "                print(f\"No hulls detected by YOLO for {test_img}. Skipping inference for this image.\")\n",
        "                continue # Skip to the next image if no hulls are found\n",
        "        else:\n",
        "            # For quick test, make a center box covering 50% of the image width and 30% of height\n",
        "            try:\n",
        "                im = Image.open(test_img).convert('RGB')\n",
        "                W, H = im.size\n",
        "                w = int(0.5 * W)\n",
        "                h = int(0.3 * H)\n",
        "                x = int((W - w) / 2)\n",
        "                y = int((H - h) / 2)\n",
        "                hulls = [[x, y, w, h]]\n",
        "                if w <= 0 or h <= 0: # Check for invalid box dimensions\n",
        "                     print(f\"Generated invalid hull box dimensions for {test_img}. Skipping inference.\")\n",
        "                     continue\n",
        "            except Exception as e:\n",
        "                 print(f\"Error processing image {test_img}: {e}. Skipping inference.\")\n",
        "                 continue\n",
        "\n",
        "\n",
        "        preds = infer_stage_b(model, test_img, hulls, device=DEVICE, cone_cfg=CONE_CFG)\n",
        "\n",
        "        # Generate a unique output path for each image\n",
        "        img_name = os.path.basename(test_img)\n",
        "        out_path = os.path.join(output_dir, f\"masttip_prediction_result_{os.path.splitext(img_name)[0]}.jpg\")\n",
        "\n",
        "        draw_predictions(test_img, preds, out_path)\n",
        "else:\n",
        "    print(\"No images available to test.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsHVts-EfFm1",
        "outputId": "26e53dce-2ea9-4d29-b3cd-84e029228e0b"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test images: ['/gdrive/MyDrive/boats/yolo/images/train/1.png', '/gdrive/MyDrive/boats/yolo/images/train/10.png', '/gdrive/MyDrive/boats/yolo/images/train/1003.png']\n",
            "\n",
            "Processing image 1/3: /gdrive/MyDrive/boats/yolo/images/train/1.png\n",
            "Saved: /tmp/mastertip_results/masttip_prediction_result_1.jpg\n",
            "\n",
            "Processing image 2/3: /gdrive/MyDrive/boats/yolo/images/train/10.png\n",
            "Saved: /tmp/mastertip_results/masttip_prediction_result_10.jpg\n",
            "\n",
            "Processing image 3/3: /gdrive/MyDrive/boats/yolo/images/train/1003.png\n",
            "Saved: /tmp/mastertip_results/masttip_prediction_result_1003.jpg\n"
          ]
        }
      ]
    }
  ]
}