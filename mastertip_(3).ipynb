{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jooalee64/mastertip/blob/main/mastertip_(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1148b9f2",
        "outputId": "45760286-493a-4a07-e7eb-833a0e81a1fa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mastertip_"
      ],
      "metadata": {
        "id": "h0hKDKSVUdd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "mastip2"
      ],
      "metadata": {
        "id": "-kE7hOOYbqBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: if you're on Colab, mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')\n",
        "\n",
        "# (1) Imports\n",
        "import os, math, json, glob, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision.ops import roi_align\n",
        "from torchvision.transforms import functional as TF\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "# (2) User paths — EDIT HERE if needed\n",
        "ROOT_DIR = \"/gdrive/MyDrive/boats\"\n",
        "print(\"ROOT_DIR:\", ROOT_DIR)\n",
        "\n",
        "# Image directories\n",
        "IMAGES_DIR = os.path.join(ROOT_DIR, \"yolo\", \"images\")\n",
        "TRAIN_IMAGES_DIR = os.path.join(IMAGES_DIR, \"train\")\n",
        "VAL_IMAGES_DIR   = os.path.join(IMAGES_DIR, \"val\")\n",
        "TEST_IMAGES_DIR  = os.path.join(IMAGES_DIR, \"test\")\n",
        "\n",
        "# Label directories — choose ONE of the following (uncomment as needed)\n",
        "\n",
        "# A) Standard YOLO structure\n",
        "LABELS_DIR = os.path.join(ROOT_DIR, \"yolo\", \"labels\")\n",
        "\n",
        "# B) Your custom storage path (uncomment to use)\n",
        "# LABELS_DIR = os.path.join(ROOT_DIR, \"yolo\", \"labels-storage\", \"storage-a-labels\", \"hull-labels\")\n",
        "\n",
        "TRAIN_LABELS_DIR = os.path.join(LABELS_DIR, \"train\")\n",
        "VAL_LABELS_DIR   = os.path.join(LABELS_DIR, \"val\")\n",
        "TEST_LABELS_DIR  = os.path.join(LABELS_DIR, \"test\")\n",
        "\n",
        "print(\"LABELS_DIR:\", LABELS_DIR)\n",
        "\n",
        "# Class IDs (adjust if your dataset uses different ids)\n",
        "HULL_CLASS_ID = 0  # hull box\n",
        "TIP_CLASS_ID  = 1  # tip encoded as tiny box (we use its center)\n",
        "\n",
        "# Training hyperparams (edit as you like)\n",
        "MAX_SIDE    = 1024\n",
        "BATCH_SIZE  = 4\n",
        "EPOCHS      = 5\n",
        "LR          = 1e-3\n",
        "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Cone-mask config for Stage-B\n",
        "CONE_CFG = dict(base_y_ratio=0.75, apex_y_ratio=0.05, base_width_ratio=0.35)\n",
        "\n",
        "# Utility\n",
        "def set_seed(seed: int = 1337):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(1337)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvvs_2ySbviS",
        "outputId": "c5d4d922-547d-43e7-e94d-339626271564"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROOT_DIR: /gdrive/MyDrive/boats\n",
            "LABELS_DIR: /gdrive/MyDrive/boats/yolo/labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils & transforms\n",
        "\n",
        "def to_tensor(img: Image.Image) -> torch.Tensor:\n",
        "    return TF.to_tensor(img)\n",
        "\n",
        "def resize_keep_aspect(img: Image.Image, max_side: int = 1024) -> Tuple[Image.Image, float]:\n",
        "    w, h = img.size\n",
        "    scale = 1.0\n",
        "    if max(w, h) > max_side:\n",
        "        scale = max_side / float(max(w, h))\n",
        "        img = img.resize((int(round(w * scale)), int(round(h * scale))), Image.BILINEAR)\n",
        "    return img, scale\n"
      ],
      "metadata": {
        "id": "1iHmkstpbxnI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets (JSONL & YOLO dirs)\n",
        "\n",
        "@dataclass\n",
        "class Sample:\n",
        "    image_path: str\n",
        "    hulls: List[List[float]]\n",
        "    tips: List[List[float]]\n",
        "\n",
        "class MastTipJsonl(Dataset):\n",
        "    def __init__(self, jsonl_path: str, max_side: int = 1024, training: bool = True):\n",
        "        self.items: List[Sample] = []\n",
        "        with open(jsonl_path, 'r') as f:\n",
        "            for line in f:\n",
        "                j = json.loads(line)\n",
        "                self.items.append(Sample(j['image'], j['hulls'], j['tips']))\n",
        "        self.max_side = max_side\n",
        "        self.training = training\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        it = self.items[idx]\n",
        "        img = Image.open(it.image_path).convert('RGB')\n",
        "        img, scale = resize_keep_aspect(img, self.max_side)\n",
        "        w, h = img.size\n",
        "        img_t = to_tensor(img)\n",
        "\n",
        "        hulls = np.array(it.hulls, dtype=np.float32) * scale\n",
        "        tips = np.array(it.tips, dtype=np.float32) * scale\n",
        "\n",
        "        if self.training and random.random() < 0.5:\n",
        "            img_t = torch.flip(img_t, dims=[2])\n",
        "            hulls[:, 0] = (w - hulls[:, 0] - hulls[:, 2])\n",
        "            tips[:, 0] = (w - tips[:, 0])\n",
        "\n",
        "        target = {'hulls': torch.from_numpy(hulls),\n",
        "                  'tips': torch.from_numpy(tips),\n",
        "                  'scale': scale,\n",
        "                  'size': (h, w)}\n",
        "        return img_t, target\n",
        "\n",
        "def _parse_yolo_label(label_path: str, img_w: int, img_h: int):\n",
        "    hulls: List[List[float]] = []\n",
        "    tips_xy: List[List[float]] = []\n",
        "    if not os.path.exists(label_path):\n",
        "        return hulls, tips_xy\n",
        "    with open(label_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                continue\n",
        "            cls = int(float(parts[0]))\n",
        "            x_c, y_c, w_n, h_n = map(float, parts[1:])\n",
        "            x_c *= img_w; y_c *= img_h; w = w_n * img_w; h = h_n * img_h\n",
        "            x = x_c - w / 2.0; y = y_c - h / 2.0\n",
        "            if cls == HULL_CLASS_ID:\n",
        "                hulls.append([x, y, w, h])\n",
        "            elif cls == TIP_CLASS_ID:\n",
        "                tips_xy.append([x_c, y_c])  # center as tip\n",
        "    return hulls, tips_xy\n",
        "\n",
        "class MastTipYoloDirs(Dataset):\n",
        "    def __init__(self, images_dir: str, labels_dir: str, max_side: int = 1024, training: bool = True):\n",
        "        self.images = []\n",
        "        exts = ('*.jpg', '*.jpeg', '*.png', '*.bmp')\n",
        "        for e in exts:\n",
        "            self.images.extend(sorted(glob.glob(os.path.join(images_dir, e))))\n",
        "        self.labels_dir = labels_dir\n",
        "        self.max_side = max_side\n",
        "        self.training = training\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        label_path = os.path.join(self.labels_dir, base + '.txt')\n",
        "\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        w0, h0 = img.size\n",
        "        hulls, tips = _parse_yolo_label(label_path, w0, h0)\n",
        "\n",
        "        # naive 1:1 pairing\n",
        "        n = min(len(hulls), len(tips))\n",
        "        hulls = hulls[:n]; tips = tips[:n]\n",
        "        if n == 0:\n",
        "            img, _ = resize_keep_aspect(img, self.max_side)\n",
        "            return to_tensor(img), {'hulls': torch.zeros((0,4), dtype=torch.float32),\n",
        "                                    'tips': torch.zeros((0,2), dtype=torch.float32),\n",
        "                                    'scale': 1.0, 'size': img.size[::-1]}\n",
        "\n",
        "        img, scale = resize_keep_aspect(img, self.max_side)\n",
        "        w, h = img.size\n",
        "        img_t = to_tensor(img)\n",
        "\n",
        "        hulls = (np.array(hulls, dtype=np.float32) * scale)\n",
        "        tips  = (np.array(tips,  dtype=np.float32) * scale)\n",
        "\n",
        "        if self.training and random.random() < 0.5:\n",
        "            img_t = torch.flip(img_t, dims=[2])\n",
        "            hulls[:, 0] = (w - hulls[:, 0] - hulls[:, 2])\n",
        "            tips[:, 0]  = (w - tips[:, 0])\n",
        "\n",
        "        target = {'hulls': torch.from_numpy(hulls),\n",
        "                  'tips': torch.from_numpy(tips),\n",
        "                  'scale': scale,\n",
        "                  'size': (h, w)}\n",
        "        return img_t, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs = [b[0] for b in batch]\n",
        "    tgts = [b[1] for b in batch]\n",
        "    return imgs, tgts\n"
      ],
      "metadata": {
        "id": "zDTiiTyLbzNr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Components (Backbone, Tip head, Cone mask, StageB)\n",
        "\n",
        "class BackboneResNet50(nn.Module):\n",
        "    def __init__(self, pretrained: bool = True):\n",
        "        super().__init__()\n",
        "        m = resnet50(weights=(ResNet50_Weights.IMAGENET1K_V2 if pretrained else None))\n",
        "        self.stem = nn.Sequential(m.conv1, m.bn1, m.relu, m.maxpool)\n",
        "        self.layer1 = m.layer1\n",
        "        self.layer2 = m.layer2\n",
        "        self.layer3 = m.layer3  # stride 16\n",
        "        self.proj = nn.Conv2d(1024, 256, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "def gaussian_nll_2d(mu, log_sigma, rho_raw, y, reduction=\"mean\", eps=1e-6):\n",
        "    sigma = torch.exp(log_sigma) + eps\n",
        "    rho = torch.tanh(rho_raw).clamp(-0.999, 0.999)\n",
        "    dx = (y[:, 0] - mu[:, 0]) / sigma[:, 0]\n",
        "    dy = (y[:, 1] - mu[:, 1]) / sigma[:, 1]\n",
        "    rho_sq = rho[:, 0] ** 2\n",
        "    one_m_rho_sq = 1.0 - rho_sq + eps\n",
        "    q = (dx * dx - 2 * rho[:, 0] * dx * dy + dy * dy) / one_m_rho_sq\n",
        "    log_det = 2 * log_sigma[:, 0] + 2 * log_sigma[:, 1] + torch.log(one_m_rho_sq)\n",
        "    nll = 0.5 * (log_det + q + math.log(2.0 * math.pi))\n",
        "    return nll.mean() if reduction == \"mean\" else nll\n",
        "\n",
        "class TipHeadGaussian(nn.Module):\n",
        "    def __init__(self, in_channels=256, hidden=128, roi_size=28): # Added roi_size\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(hidden, hidden, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden, 5)\n",
        "\n",
        "        # Initialize bias for mu prediction to the center of the ROI\n",
        "        with torch.no_grad():\n",
        "            # The first two outputs of fc are mu_x and mu_y\n",
        "            self.fc.bias[:2].fill_(roi_size / 2.0) # Initialize mu bias to center\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x).flatten(1)\n",
        "        out = self.fc(h)\n",
        "        mu = out[:, :2]\n",
        "        log_sigma = out[:, 2:4]\n",
        "        rho_raw = out[:, 4:5]\n",
        "        return mu, log_sigma, rho_raw\n",
        "\n",
        "def build_cone_mask(h, w, base_y, base_xc, base_half_width, apex_y):\n",
        "    yy, xx = torch.meshgrid(torch.arange(h), torch.arange(w), indexing='ij')\n",
        "    denom = max(1, (base_y - apex_y))\n",
        "    t = (yy.float() - apex_y) / denom\n",
        "    t = torch.clamp(t, 0.0, 1.0)\n",
        "    halfw = (1.0 - t) * 1 + t * base_half_width\n",
        "    mask = (torch.abs(xx.float() - base_xc) <= halfw) & (yy >= apex_y) & (yy <= base_y)\n",
        "    return mask.float()\n",
        "\n",
        "class StageBTipNet(nn.Module):\n",
        "    def __init__(self, roi_out=28, pretrained_backbone=True):\n",
        "        super().__init__()\n",
        "        self.backbone = BackboneResNet50(pretrained=pretrained_backbone)\n",
        "        self.roi_size = roi_out # Store roi_size\n",
        "        self.tip_head = TipHeadGaussian(in_channels=256, hidden=128, roi_size=self.roi_size) # Pass roi_size\n",
        "        self.spatial_scale = 1.0 / 16.0\n",
        "\n",
        "\n",
        "    def forward(self, images: List[torch.Tensor], rois_xywh: List[torch.Tensor], cone_cfg: Dict[str, float]):\n",
        "        device = images[0].device\n",
        "        imgs = torch.stack(images, dim=0)\n",
        "        feats = self.backbone(imgs)\n",
        "\n",
        "        roi_list = []\n",
        "        batch_index = []\n",
        "        for b, boxes in enumerate(rois_xywh):\n",
        "            if boxes.numel() == 0:\n",
        "                continue\n",
        "            xyxy = boxes.clone()\n",
        "            xyxy[:, 2] = xyxy[:, 0] + xyxy[:, 2]\n",
        "            xyxy[:, 3] = xyxy[:, 1] + xyxy[:, 3]\n",
        "            bidx = torch.full((xyxy.size(0), 1), b, device=xyxy.device)\n",
        "            roi_list.append(torch.cat([bidx, xyxy], dim=1))\n",
        "            batch_index.extend([b] * xyxy.size(0))\n",
        "        if len(roi_list) == 0:\n",
        "            return None\n",
        "        rois = torch.cat(roi_list, dim=0)\n",
        "\n",
        "        crops = roi_align(\n",
        "            feats, rois,\n",
        "            output_size=(self.roi_size // 4, self.roi_size // 4),\n",
        "            spatial_scale=self.spatial_scale,\n",
        "            sampling_ratio=2,\n",
        "            aligned=True,\n",
        "        )\n",
        "        crops_up = F.interpolate(crops, size=(self.roi_size, self.roi_size), mode='bilinear', align_corners=False)\n",
        "\n",
        "        N = crops_up.size(0)\n",
        "        masked = []\n",
        "        base_y_ratio = cone_cfg.get('base_y_ratio', 0.75)\n",
        "        apex_y_ratio = cone_cfg.get('apex_y_ratio', 0.05)\n",
        "        base_width_ratio = cone_cfg.get('base_width_ratio', 0.35)\n",
        "        for i in range(N):\n",
        "            H, W = self.roi_size, self.roi_size\n",
        "            base_y = int(round(base_y_ratio * (H - 1)))\n",
        "            apex_y = int(round(apex_y_ratio * (H - 1)))\n",
        "            base_xc = W // 2\n",
        "            base_halfw = int(round(base_width_ratio * (W / 2)))\n",
        "            m = build_cone_mask(H, W, base_y, base_xc, base_half_width=base_halfw, apex_y=apex_y).to(crops_up.device)\n",
        "            masked.append(crops_up[i] * m.unsqueeze(0))\n",
        "        masked = torch.stack(masked, dim=0)\n",
        "\n",
        "\n",
        "        mu, log_sigma, rho_raw = self.tip_head(masked)\n",
        "        aux = {'masked': masked, 'crops': crops_up, 'rois_batched': rois, 'batch_index': torch.tensor(batch_index, device=device)}\n",
        "        return mu, log_sigma, rho_raw, aux"
      ],
      "metadata": {
        "id": "905l_S5bb1C2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training utilities\n",
        "\n",
        "def to_roi_coords(points_xy: torch.Tensor, boxes_xywh: torch.Tensor, roi_size: int) -> torch.Tensor:\n",
        "    x = (points_xy[:, 0] - boxes_xywh[:, 0]) / boxes_xywh[:, 2] * roi_size\n",
        "    y = (points_xy[:, 1] - boxes_xywh[:, 1]) / boxes_xywh[:, 3] * roi_size\n",
        "    return torch.stack([x, y], dim=1)\n",
        "\n",
        "def train_stage_b(model: 'StageBTipNet', loader: DataLoader, optimizer: torch.optim.Optimizer,\n",
        "                  device: str = 'cuda', epochs: int = 5, cone_cfg: Optional[Dict[str, float]] = None,\n",
        "                  log_every: int = 20):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    cone_cfg = cone_cfg or {}\n",
        "    global_step = 0\n",
        "    for ep in range(epochs):\n",
        "        for imgs, tgts in loader:\n",
        "            imgs = [im.to(device) for im in imgs]\n",
        "            rois = [t['hulls'].to(device).float() for t in tgts]\n",
        "            tips = [t['tips'].to(device).float() for t in tgts]\n",
        "            if sum([r.size(0) for r in rois]) == 0:\n",
        "                continue\n",
        "            tips_flat = torch.cat(tips, dim=0)\n",
        "            rois_flat = torch.cat(rois, dim=0)\n",
        "\n",
        "            out = model(imgs, rois, cone_cfg)\n",
        "            if out is None:\n",
        "                continue\n",
        "            mu, log_sigma, rho_raw, aux = out\n",
        "\n",
        "            y_roi = to_roi_coords(tips_flat.to(mu.device), rois_flat.to(mu.device), model.roi_size)\n",
        "            loss = gaussian_nll_2d(mu, log_sigma, rho_raw, y_roi, reduction='mean')\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            if global_step % log_every == 0:\n",
        "                print(f\"epoch {ep:02d} step {global_step:05d} | loss {loss.item():.4f}\")\n",
        "            global_step += 1\n"
      ],
      "metadata": {
        "id": "TsHnsen9b3gc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference helpers & visualization\n",
        "\n",
        "def run_yolo_detect(image_path: str, conf: float = 0.25) -> List[List[float]]:\n",
        "    \"\"\"Optional: Use Ultralytics YOLO if installed. Returns hull boxes [x,y,w,h].\"\"\"\n",
        "    try:\n",
        "        from ultralytics import YOLO\n",
        "    except Exception:\n",
        "        print(\"[INFO] Ultralytics not installed; returning empty boxes.\")\n",
        "        return []\n",
        "    # Load a box detector; replace with your fine-tuned hull model path\n",
        "    # Using the user-provided path for the hull detector model\n",
        "    model_path = \"/gdrive/MyDrive/boats/yolo/models/hull-detector/yolo-hull-detector.pt\"\n",
        "    try:\n",
        "        model = YOLO(model_path)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Could not load YOLO model from {model_path}: {e}\")\n",
        "        print(\"Returning empty boxes.\")\n",
        "        return []\n",
        "\n",
        "    res = model(image_path, conf=conf, verbose=False)[0]\n",
        "    boxes_xyxy = res.boxes.xyxy.cpu().numpy().astype(np.float32)\n",
        "    hulls = []\n",
        "    for x1, y1, x2, y2 in boxes_xyxy:\n",
        "        hulls.append([float(x1), float(y1), float(x2 - x1), float(y2 - y1)])\n",
        "    return hulls\n",
        "\n",
        "def infer_stage_b(model: 'StageBTipNet', image_path: str, hulls_xywh: List[List[float]],\n",
        "                  device: str = 'cuda', cone_cfg: Optional[Dict[str, float]] = None) -> List[Dict]:\n",
        "    model.eval()\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img, _ = resize_keep_aspect(img, max_side=1024)\n",
        "    img_t = to_tensor(img).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model([img_t], [torch.tensor(hulls_xywh, device=device).float()], cone_cfg or {})\n",
        "        if out is None:\n",
        "            return []\n",
        "        mu, log_sigma, rho_raw, aux = out\n",
        "        # Convert mu from RoI coords to image coords\n",
        "        rois = aux['rois_batched'][:, 1:5]\n",
        "        boxes = rois.clone()\n",
        "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
        "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
        "        mu_img_x = mu[:, 0] / model.roi_size * boxes[:, 2] + boxes[:, 0]\n",
        "        mu_img_y = mu[:, 1] / model.roi_size * boxes[:, 3] + boxes[:, 1]\n",
        "        sigma = torch.exp(log_sigma)\n",
        "        rho = torch.tanh(rho_raw).clamp(-0.999, 0.999)\n",
        "        out_list = []\n",
        "        for i in range(mu.size(0)):\n",
        "            out_list.append({'tip_xy': (float(mu_img_x[i].item()), float(mu_img_y[i].item())),\n",
        "                             'sigma': (float(sigma[i, 0].item()), float(sigma[i, 1].item())),\n",
        "                             'rho': float(rho[i, 0].item()),\n",
        "                             'box_xywh': [float(x) for x in boxes[i].tolist()]})\n",
        "        return out_list\n",
        "\n",
        "def draw_predictions(image_path: str, preds: List[Dict], save_path: str):\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    dr = ImageDraw.Draw(img)\n",
        "    for p in preds:\n",
        "        x, y, w, h = p['box_xywh']\n",
        "        dr.rectangle([x, y, x + w, y + h], outline=(0, 255, 0), width=2)\n",
        "        tx, ty = p['tip_xy']; r = 3\n",
        "        dr.ellipse([tx - r, ty - r, tx + r, ty + r], outline=(255, 0, 0), width=2)\n",
        "    img.save(save_path)\n",
        "    print(\"Saved:\", save_path)"
      ],
      "metadata": {
        "id": "S1eBN8N3b5x8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build dataset & train Stage-B using YOLO dirs\n",
        "\n",
        "train_ds = MastTipYoloDirs(images_dir=TRAIN_IMAGES_DIR, labels_dir=TRAIN_LABELS_DIR, max_side=MAX_SIDE, training=True)\n",
        "print(\"Num train images:\", len(train_ds))\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "model = StageBTipNet(roi_out=28, pretrained_backbone=True).to(DEVICE)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "\n",
        "train_stage_b(model, train_loader, opt, device=DEVICE, epochs=EPOCHS, cone_cfg=CONE_CFG, log_every=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywPq-ixXb7ph",
        "outputId": "d6126cde-705b-49cd-fd16-e943239fe1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num train images: 1050\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 119MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test inference on multiple images.\n",
        "# 1) Set a list of image paths from your dataset:\n",
        "test_images = []\n",
        "\n",
        "# e.g., pick the first few training images if available\n",
        "if len(train_ds) > 0:\n",
        "    # You can adjust the number of images to test here (e.g., [:5] for the first 5)\n",
        "    test_images = train_ds.images[:5]\n",
        "\n",
        "print(\"Test images:\", test_images)\n",
        "\n",
        "# 2) Choose how to get hulls: (A) use YOLO detect (needs ultralytics), or (B) make a center box\n",
        "USE_YOLO = True\n",
        "\n",
        "output_dir = \"/tmp/mastertip_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "if test_images:\n",
        "    for i, test_img in enumerate(test_images):\n",
        "        print(f\"\\nProcessing image {i+1}/{len(test_images)}: {test_img}\")\n",
        "        if USE_YOLO:\n",
        "            hulls = run_yolo_detect(test_img)  # list of [x,y,w,h]\n",
        "            if not hulls:\n",
        "                print(f\"No hulls detected by YOLO for {test_img}. Skipping inference for this image.\")\n",
        "                continue # Skip to the next image if no hulls are found\n",
        "        else:\n",
        "            # For quick test, make a center box covering 50% of the image width and 30% of height\n",
        "            try:\n",
        "                im = Image.open(test_img).convert('RGB')\n",
        "                W, H = im.size\n",
        "                w = int(0.5 * W)\n",
        "                h = int(0.3 * H)\n",
        "                x = int((W - w) / 2)\n",
        "                y = int((H - h) / 2)\n",
        "                hulls = [[x, y, w, h]]\n",
        "                if w <= 0 or h <= 0: # Check for invalid box dimensions\n",
        "                     print(f\"Generated invalid hull box dimensions for {test_img}. Skipping inference.\")\n",
        "                     continue\n",
        "            except Exception as e:\n",
        "                 print(f\"Error processing image {test_img}: {e}. Skipping inference.\")\n",
        "                 continue\n",
        "\n",
        "\n",
        "        preds = infer_stage_b(model, test_img, hulls, device=DEVICE, cone_cfg=CONE_CFG)\n",
        "\n",
        "        # Generate a unique output path for each image\n",
        "        img_name = os.path.basename(test_img)\n",
        "        out_path = os.path.join(output_dir, f\"masttip_prediction_result_{os.path.splitext(img_name)[0]}.jpg\")\n",
        "\n",
        "        draw_predictions(test_img, preds, out_path)\n",
        "else:\n",
        "    print(\"No images available to test.\")"
      ],
      "metadata": {
        "id": "DsHVts-EfFm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "veOc2JfnjDWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef127315"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Ellipse\n",
        "import matplotlib.transforms as transforms\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Helper function to draw a confidence ellipse\n",
        "def plot_confidence_ellipse(mu, sigma, rho, ax, n_std=2.0, facecolor='none', edgecolor='red', **kwargs):\n",
        "    \"\"\"\n",
        "    Plots an ellipse representing the covariance of a 2D Gaussian distribution.\n",
        "\n",
        "    mu: mean (x, y)\n",
        "    sigma: standard deviations (sigma_x, sigma_y)\n",
        "    rho: correlation coefficient\n",
        "    ax: matplotlib axes object\n",
        "    n_std: number of standard deviations for the ellipse size\n",
        "    \"\"\"\n",
        "    cov = np.array([[sigma[0]**2, rho * sigma[0] * sigma[1]],\n",
        "                    [rho * sigma[0] * sigma[1], sigma[1]**2]])\n",
        "\n",
        "    # Eigenvalues and eigenvectors of the covariance matrix\n",
        "    pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n",
        "    ell_radius_x = np.sqrt(1 + pearson)\n",
        "    ell_radius_y = np.sqrt(1 - pearson)\n",
        "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2, facecolor=facecolor, edgecolor=edgecolor, **kwargs)\n",
        "\n",
        "    # Calculate the rotation angle of the ellipse\n",
        "    theta = np.arctan2(cov[1, 0], cov[0, 0] - cov[1, 1]) / 2\n",
        "    angle = np.degrees(theta) # in degrees\n",
        "\n",
        "    # Scale the ellipse by the standard deviations\n",
        "    scale_x = sigma[0] * n_std\n",
        "    scale_y = sigma[1] * n_std\n",
        "\n",
        "    trans = transforms.Affine2D().rotate_deg(angle).scale(scale_x, scale_y).translate(mu[0], mu[1])\n",
        "    ellipse.set_transform(trans + ax.transData)\n",
        "    return ax.add_patch(ellipse)\n",
        "\n",
        "# --- Configuration ---\n",
        "# Select an image to visualize (e.g., the first image from the training dataset)\n",
        "if len(train_ds) > 0:\n",
        "    test_image_path = train_ds.images[0]\n",
        "    print(f\"Visualizing: {test_image_path}\")\n",
        "else:\n",
        "    print(\"No images available in train_ds for visualization.\")\n",
        "    test_image_path = None\n",
        "\n",
        "# Whether to use YOLO for hull detection or the placeholder box\n",
        "# Make sure USE_YOLO is defined in your environment (e.g., from cell DsHVts-EfFm1)\n",
        "# USE_YOLO = True # Uncomment and set if not already defined globally\n",
        "\n",
        "# --- Visualization Code ---\n",
        "if test_image_path:\n",
        "    # Get hulls (using YOLO if USE_YOLO is True, otherwise the placeholder)\n",
        "    if USE_YOLO:\n",
        "         # Make sure run_yolo_detect is defined (e.g., from cell S1eBN8N3b5x8)\n",
        "        hulls = run_yolo_detect(test_image_path)\n",
        "        if not hulls:\n",
        "            print(f\"No hulls detected by YOLO for {test_image_path}. Cannot perform Stage B inference.\")\n",
        "            hulls = [] # Ensure hulls is an empty list if no detections\n",
        "    else:\n",
        "        # Placeholder hull detection logic (from cell DsHVts-EfFm1)\n",
        "        try:\n",
        "            im_viz = Image.open(test_image_path).convert('RGB')\n",
        "            W, H = im_viz.size\n",
        "            w = int(0.5 * W)\n",
        "            h = int(0.3 * H)\n",
        "            x = int((W - w) / 2)\n",
        "            y = int((H - h) / 2)\n",
        "            hulls = [[x, y, w, h]]\n",
        "            if w <= 0 or h <= 0:\n",
        "                 print(f\"Generated invalid hull box dimensions for {test_image_path}. Cannot perform Stage B inference.\")\n",
        "                 hulls = []\n",
        "        except Exception as e:\n",
        "             print(f\"Error processing image {test_image_path}: {e}. Cannot perform Stage B inference.\")\n",
        "             hulls = []\n",
        "\n",
        "\n",
        "    # Perform Stage B inference if hulls are available\n",
        "    if hulls:\n",
        "        # Make sure infer_stage_b is defined (e.g., from cell S1eBN8N3b5x8)\n",
        "        # Make sure model and DEVICE are defined (e.g., from cell ywPq-ixXb7ph)\n",
        "        # Make sure CONE_CFG is defined (e.g., from cell Pvvs_2ySbviS)\n",
        "        preds = infer_stage_b(model, test_image_path, hulls, device=DEVICE, cone_cfg=CONE_CFG)\n",
        "\n",
        "        # Load the original image for plotting\n",
        "        img_orig = Image.open(test_image_path).convert('RGB')\n",
        "        img_np = np.array(img_orig)\n",
        "\n",
        "        # Create plot\n",
        "        fig, ax = plt.subplots(1, figsize=(12, 8))\n",
        "        ax.imshow(img_np)\n",
        "        ax.set_title(\"Predictions (Hull: Green, Tip Mean: Red Dot, Confidence Ellipse: Red)\")\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Draw predictions\n",
        "        for p in preds:\n",
        "            # Draw hull box\n",
        "            x, y, w, h = p['box_xywh']\n",
        "            rect = plt.Rectangle((x, y), w, h, linewidth=2, edgecolor='green', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "            # Draw predicted tip mean (red dot)\n",
        "            tip_x, tip_y = p['tip_xy']\n",
        "            ax.plot(tip_x, tip_y, 'ro', markersize=5)\n",
        "\n",
        "            # Draw confidence ellipse\n",
        "            plot_confidence_ellipse(p['tip_xy'], p['sigma'], p['rho'], ax, n_std=2.0, edgecolor='red', linewidth=1.5)\n",
        "\n",
        "        # Optional: Draw Ground Truth Tips for comparison\n",
        "        # This requires accessing the ground truth labels\n",
        "        try:\n",
        "            # Assuming you have the label path and _parse_yolo_label function available\n",
        "            # Need to get the original image size to scale YOLO labels\n",
        "            original_img_w, original_img_h = Image.open(test_image_path).size\n",
        "            base = os.path.splitext(os.path.basename(test_image_path))[0]\n",
        "            # Assuming LABELS_DIR and TRAIN_LABELS_DIR are defined\n",
        "            # Need to determine if this test image is from train, val, or test set\n",
        "            # For simplicity, assuming it's from train for this example\n",
        "            label_path_gt = os.path.join(TRAIN_LABELS_DIR, base + '.txt')\n",
        "\n",
        "            if os.path.exists(label_path_gt):\n",
        "                _, gt_tips = _parse_yolo_label(label_path_gt, original_img_w, original_img_h)\n",
        "                if gt_tips:\n",
        "                    print(f\"Found {len(gt_tips)} ground truth tips.\")\n",
        "                    for gt_tip_x, gt_tip_y in gt_tips:\n",
        "                         # Draw ground truth tip (e.g., as a blue cross)\n",
        "                         ax.plot(gt_tip_x, gt_tip_y, 'bx', markersize=8, markeredgewidth=2, label='Ground Truth Tip')\n",
        "                    # Add legend if ground truth tips are plotted\n",
        "                    # handles, labels = ax.get_legend_handles_labels()\n",
        "                    # unique_labels = dict(zip(labels, handles))\n",
        "                    # ax.legend(unique_labels.values(), unique_labels.keys())\n",
        "\n",
        "            else:\n",
        "                 print(f\"Ground truth label file not found for {test_image_path} at {label_path_gt}\")\n",
        "        except NameError:\n",
        "             print(\"Could not visualize ground truth tips: required variables (e.g., TRAIN_LABELS_DIR, _parse_yolo_label) not defined in this scope.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while trying to visualize ground truth tips: {e}\")\n",
        "\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"Skipping visualization as no hulls were detected or generated.\")\n",
        "\n",
        "\n",
        "\n",
        "# Save the collected results\n",
        "\n",
        "# Create the results directory if it doesn't exist\n",
        "results_dir = \"results\"\n",
        "os.makedirs(results_dir, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}